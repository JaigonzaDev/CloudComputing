# Exam 4

An e-commerce company tracks user clicks on its flagship website and performs analytics to provide near-real-time product recommendations. An Amazon EC2 instance receives data from the website and sends the data to an Amazon Aurora Database instance. Another Amazon EC2 instance continuously checks the changes in the database and executes SQL queries to provide recommendations. Now, the company wants a redesign to decouple and scale the infrastructure. The solution must ensure that data can be analyzed in real-time without any data loss even when the company sees huge traffic spikes.

What would you recommend as an AWS Certified Solutions Architect - Associate?

Leverage Amazon SQS to capture the data from the website. Configure a fleet of Amazon EC2 instances under an Auto scaling group to process messages from the Amazon SQS queue and trigger the scaling policy based on the number of pending messages in the queue. Perform real-time analytics using a third-party library on the Amazon EC2 instances

Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon QuickSight which can query the data in real time. Lastly, the analyzed feed is output into Kinesis Data Firehose to persist the data on Amazon S3

Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Firehose to persist the data on Amazon S3. Lastly, use Amazon Athena to analyze the data in real time

Leverage Amazon Kinesis Data Streams to capture the data from the website and feed it into Amazon Kinesis Data Analytics which can query the data in real time. Lastly, the analyzed feed is output into Amazon Kinesis Data Firehose to persist the data on Amazon S3

You can use Amazon Kinesis Data Streams to build custom applications that process or analyze streaming data for specialized needs. Amazon Kinesis Data Streams manages the infrastructure, storage, networking, and configuration needed to stream your data at the level of your data throughput. You don't have to worry about provisioning, deployment, or ongoing maintenance of hardware, software, or other services for your data streams.

For the given use case, you can use Amazon Kinesis Data Analytics to transform and analyze incoming streaming data from Kinesis Data Streams in real time. Kinesis Data Analytics takes care of everything required to run streaming applications continuously, and scales automatically to match the volume and throughput of your incoming data. With Amazon Kinesis Data Analytics, there are no servers to manage, no minimum fee or setup cost, and you only pay for the resources your streaming applications consume.

---

You started a new job as a solutions architect at a company that has both AWS experts and people learning AWS. Recently, a developer misconfigured a newly created Amazon RDS database which resulted in a production outage.

How can you ensure that Amazon RDS specific best practices are incorporated into a reusable infrastructure template to be used by all your AWS users?

Attach an IAM policy to interns preventing them from creating an Amazon RDS database

Store your recommendations in a custom AWS Trusted Advisor rule

Create an AWS Lambda function which sends emails when it finds misconfigured Amazon RDS databases

Use AWS CloudFormation to manage Amazon RDS databases

‚Äúreusable infrastructure template‚Äù

---

A company has grown from a small startup to an enterprise employing over 1000 people. As the team size has grown, the company has recently observed some strange behavior, with Amazon S3 buckets settings being changed regularly.

How can you figure out what's happening without restricting the rights of the users?

Implement an IAM policy to forbid users to change Amazon S3 bucket settings

Use AWS CloudTrail to analyze API calls

Implement a bucket policy requiring AWS Multi-Factor Authentication (AWS MFA) for all operations

Use Amazon S3 access logs to analyze user access using Athena

---

A CRM web application was written as a monolith in PHP and is facing scaling issues because of performance bottlenecks. The CTO wants to re-engineer towards microservices architecture and expose their application from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, yourcorp.com/profile and yourcorp.com/search. The CTO would like to expose all these URLs as HTTPS endpoints for security purposes.

As a solutions architect, which of the following would you recommend as a solution that requires MINIMAL configuration effort?

Use a wildcard Secure Sockets Layer certificate (SSL certificate)

Use an HTTP to HTTPS redirect

Change the Elastic Load Balancing (ELB) SSL Security Policy

Use Secure Sockets Layer certificate (SSL certificate) with SNI

üîπ

**SNI permite que m√∫ltiples dominios compartan la misma IP y Load Balancer**

, enviando el dominio en la negociaci√≥n TLS.

üîπ

**El Load Balancer elige el certificado correcto basado en el SNI**

.

üîπ

**Despu√©s de la conexi√≥n segura, el tr√°fico HTTP(S) se maneja normalmente**

.

---

A systems administrator is creating IAM policies and attaching them to IAM identities. After creating the necessary identity-based policies, the administrator is now creating resource-based policies.

Which is the only resource-based policy that the IAM service supports?

Access control list (ACL)

Permissions boundary

Trust policy

AWS Organizations Service Control Policies (SCP)

**Trust policy**

Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.

---

A startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high for their business requirements.

Which of the following options represents a valid cost-optimization solution?

Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs

Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to Amazon S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies

Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations

Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew reserved instances (RI). AWS Trusted advisor also suggests Amazon RDS idle database instances

 Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data. It does not recommend instance purchase options.

**Use AWS Cost Explorer Resource Optimization to get a report of Amazon EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations**

AWS Cost Explorer helps you identify under-utilized Amazon EC2 instances that may be downsized on an instance by instance basis within the same instance family, and also understand the potential impact on your AWS bill by taking into account your Reserved Instances and Savings Plans.

AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data.

---

The engineering team at an e-commerce company has been tasked with migrating to a serverless architecture. The team wants to focus on the key points of consideration when using AWS Lambda as a backbone for this architecture.

As a Solutions Architect, which of the following options would you identify as correct for the given requirement? (Select three)

By default, AWS Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once an AWS Lambda function is VPC-enabled, it will need a route through a Network Address Translation gateway (NAT gateway) in a public subnet to access public resources

AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions

Serverless architecture and containers complement each other but you cannot package and deploy AWS Lambda functions as container images

Since AWS Lambda functions can scale extremely quickly, it's a good idea to deploy a Amazon CloudWatch Alarm that notifies your team when function metrics such as¬†`ConcurrentExecutions`¬†or¬†`Invocations exceeds the expected threshold`

The bigger your deployment package, the slower your AWS Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual AWS Lambda package

If you intend to reuse code in more than one AWS Lambda function, you should consider creating an AWS Lambda Layer for the reusable code

**AWS Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of AWS Lambda functions**¬†- AWS Lambda allocates compute power in proportion to the memory you allocate to your function. This means you can over-provision memory to run your functions faster and potentially reduce your costs. However, AWS recommends that you should not over provision your function time out settings. Always understand your code performance and set a function time out accordingly. Overprovisioning function timeout often results in Lambda functions running longer than expected and unexpected costs.

---

As a solutions architect, you have created a solution that utilizes an Application Load Balancer with stickiness and an Auto Scaling Group (ASG). The Auto Scaling Group spans across 2 Availability Zones (AZs).¬†`AZ-A`¬†has 3 Amazon EC2 instances and¬†`AZ-B`¬†has 4 Amazon EC2 instances. The Auto Scaling Group is about to go into a scale-in event due to the triggering of a Amazon CloudWatch alarm.

What will happen under the default Auto Scaling Group configuration?

A random instance will be terminated in¬†`AZ-B`

A random instance in the¬†`AZ-A`¬†will be terminated

An instance in the¬†`AZ-A`¬†will be created

The instance with the oldest launch template or launch configuration will be terminated in¬†`AZ-B`

- SCALE-IN = REDUCIR EL NUMERO DE INSTANCIAS

The default termination policy behavior is as follows: 1. Determine which Availability Zones (Azs) have the most instances and at least one instance that is not protected from scale-in. 2. Determine which instances to terminate to align the remaining instances to the allocation strategy for the On-Demand or Spot Instance that is terminating. 3. Determine whether any of the instances use the oldest launch template or configuration: 3.a. Determine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration. 3.b. Determine whether any of the instances use the oldest launch configuration. 4. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour.

---

An e-commerce company has copied 1 petabyte of data from its on-premises data center to an Amazon S3 bucket in the¬†`us-west-1`¬†Region using an AWS Direct Connect link. The company now wants to set up a one-time copy of the data to another Amazon S3 bucket in the¬†`us-east-1`¬†Region. The on-premises data center does not allow the use of AWS Snowball.

As a Solutions Architect, which of the following options can be used to accomplish this goal? (Select two)

Set up Amazon S3 Transfer Acceleration (Amazon S3TA) to copy objects across Amazon S3 buckets in different Regions using S3 console

Use AWS Snowball Edge device to copy the data from one Region to another Region

Set up Amazon S3 batch replication to copy objects across Amazon S3 buckets in another Region using S3 console and then delete the replication configuration

Copy data from the source Amazon S3 bucket to a target Amazon S3 bucket using the S3 console

Copy data from the source bucket to the destination bucket using the aws S3 sync command

El comando `aws s3 sync` es excelente para **mantener datos actualizados** entre buckets, pero **no es eficiente** para mover una gran cantidad de datos de una sola vez, especialmente en distintas regiones. Aqu√≠ es donde entra **S3 Batch Replication**

Amazon S3 Transfer Acceleration (Amazon S3TA) is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. You cannot use Transfer Acceleration to copy objects across Amazon S3 buckets in different Regions using Amazon S3 console.

---

An Elastic Load Balancer has marked all the Amazon EC2 instances in the target group as unhealthy. Surprisingly, when a developer enters the IP address of the Amazon EC2 instances in the web browser, he can access the website.

What could be the reason the instances are being marked as unhealthy? (Select two)

You need to attach elastic IP address (EIP) to the Amazon EC2 instances

Your web-app has a runtime that is not supported by the Application Load Balancer

The Amazon Elastic Block Store (Amazon EBS) volumes have been improperly mounted

The security group of the Amazon EC2 instance does not allow for traffic from the security group of the Application Load Balancer

The route for the health check is misconfigured

- El **ALB usa una ruta HTTP para verificar la salud** de las instancias (ejemplo: `/health` o `/`).
- Si la ruta no es correcta o la aplicaci√≥n **no responde con un 200 OK**, el ALB marcar√° las instancias como **unhealthy**.

You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.

---

A retail company is using AWS Site-to-Site VPN connections for secure connectivity to its AWS cloud resources from its on-premises data center. Due to a surge in traffic across the VPN connections to the AWS cloud, users are experiencing slower VPN connectivity.

Which of the following options will maximize the VPN throughput?

Create an AWS Transit Gateway with equal cost multipath routing and add additional VPN tunnels

Use Transfer Acceleration for the VPN connection to maximize the throughput

Use AWS Global Accelerator for the VPN connection to maximize the throughput

Create a virtual private gateway with equal cost multipath routing and multiple channels

 A virtual private gateway is the VPN endpoint on the Amazon side of your Site-to-Site VPN connection that can be attached to a single VPC. A virtual private gateway does not support equal cost multi-path (ECMP) routing

AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal cost multi-path (ECMP) routing support over multiple VPN tunnels. A single VPN tunnel still has a maximum throughput of 1.25 Gbps. If you establish multiple VPN tunnels to an ECMP-enabled transit gateway, it can scale beyond the default maximum limit of 1.25 Gbps. You also must enable the dynamic routing option on your transit gateway to be able to take advantage of ECMP for scalability.

---

The development team at a social media company wants to handle some complicated queries such as "What are the number of likes on the videos that have been posted by friends of a user A?".

As a solutions architect, which of the following AWS database services would you suggest as the BEST fit to handle such use cases?

Amazon Aurora

Amazon OpenSearch Service

Amazon Redshift

Amazon Neptune

Amazon **Neptune** es una base de datos **gr√°fica** totalmente gestionada. Las bases de datos gr√°ficas son especialmente poderosas cuando se trata de consultas que implican **relaciones complejas** entre diferentes entidades. En el caso planteado, la consulta sobre "el n√∫mero de likes en videos publicados por amigos de un usuario A" implica varias relaciones entre **usuarios**, **videos** y **likes**.

### **¬øPor qu√© las bases gr√°ficas son adecuadas?**

1. **Consultas complejas sobre relaciones**: En las bases de datos gr√°ficas, puedes modelar f√°cilmente **nodos (entidades)** como **usuarios** y **videos**, y **aristas (relaciones)** como **amistades** o **likes**.
    - Por ejemplo, un nodo podr√≠a ser el **usuario A**.
    - Las **amistades** entre usuarios pueden representarse como aristas entre esos nodos.
    - Los **likes** en los videos pueden ser aristas entre los nodos de los videos y los usuarios que dieron el "like".
2. **Consultas interactivas**: Amazon Neptune est√° dise√±ado para manejar consultas **altamente interactivas** y con alta **latencia baja**, lo que es crucial cuando se manejan grandes vol√∫menes de datos de redes sociales, como **amistades** y **acciones de usuarios**.
3. **Rendimiento en consultas relacionadas**: Las bases gr√°ficas permiten hacer consultas como "los amigos de A" y luego, "cu√°ntos de ellos han dado like a un video", de forma eficiente, ya que la estructura misma de la base de datos est√° optimizada para explorar y analizar relaciones entre los nodos.

---