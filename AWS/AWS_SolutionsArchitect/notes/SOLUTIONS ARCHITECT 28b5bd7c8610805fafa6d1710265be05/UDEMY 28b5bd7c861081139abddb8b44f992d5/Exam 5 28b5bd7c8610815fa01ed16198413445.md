# Exam 5

A company hires experienced specialists to analyze the customer service calls attended by its call center representatives. Now, the company wants to move to AWS Cloud and is looking at an automated solution to analyze customer service calls for sentiment analysis via ad-hoc SQL queries.

As a Solutions Architect, which of the following solutions would you recommend?

Use Amazon Transcribe to convert audio files to text and Amazon Quicksight to perform SQL based analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for reporting purposes

Use Amazon Transcribe to convert audio files to text and Amazon Athena to perform SQL based analysis to understand the underlying customer sentiments

Use Amazon Kinesis Data Streams to read the audio files and Amazon Alexa to convert them into text. Amazon Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output

Use Amazon Kinesis Data Streams to read the audio files and machine learning (ML) algorithms to convert the audio files into text and run customer sentiment analysis

[https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena](https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena)

La idea sería transcribirlo con Transcribe luego realizar querys con athena desde el s3 e integrarlo con Comprehend para en análisis de sentimientos el cual se puede integrar con athena.

---

A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon Simple Storage Service (Amazon S3) to meet compliance guidelines. The engineering team has hired you as a solutions architect to build a solution for this requirement.

Can you help the team identify the INCORRECT option from the choices below?

**Amazon S3 can encrypt object metadata by using Server-Side Encryption**

**Amazon S3 can protect data at rest using Client-Side Encryption**

**Amazon S3 can encrypt data in transit using HTTPS (TLS)**

**Amazon S3 can protect data at rest using Server-Side Encryption**

Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata.

---

A multi-national company is looking at optimizing their AWS resources across various countries and regions. They want to understand the best practices on cost optimization, performance, and security for their system architecture spanning across multiple business units.

Which AWS service is the best fit for their requirements?

AWS Config

AWS Management Console

AWS Trusted Advisor

AWS Systems Manager

AWS Trusted Advisor is an online tool that draws upon best practices learned from AWS’s aggregated operational history of serving hundreds of thousands of AWS customers. AWS Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It scans your AWS infrastructure and compares it to AWS Best practices in five categories (Cost Optimization, Performance, Security, Fault Tolerance, Service limits) and then provides recommendations.

**AWS Config** - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”. It does not offer any feedback about architectural best practices.

---

A company wants to publish an event into an Amazon Simple Queue Service (Amazon SQS) queue whenever a new object is uploaded on Amazon S3.

Which of the following statements are true regarding this functionality?

**Neither Standard Amazon SQS queue nor FIFO SQS queue are allowed as an Amazon S3 event notification destination**

Only FIFO Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas Standard SQS queue is not allowed

Both Standard Amazon SQS queue and FIFO SQS queue are allowed as an Amazon S3 event notification destination

Only Standard Amazon SQS queue is allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed

---

A junior developer is learning to build websites using HTML, CSS, and JavaScript. He has created a static website and then deployed it on Amazon S3. Now he can't seem to figure out the endpoint for his super cool website.

As a solutions architect, can you help him figure out the allowed formats for the Amazon S3 website endpoints? (Select two)

**Tu selección es incorrecta**

**http://s3-website.Region.bucket-name.amazonaws.com**

**http://bucket-name.s3-website.Region.amazonaws.com**

**http://bucket-name.s3-website-Region.amazonaws.com**

**http://s3-website-Region.bucket-name.amazonaws.com**

**http://bucket-name.Region.s3-website.amazonaws.com**

---

An Internet-of-Things (IoT) company is looking for a database solution on AWS Cloud that has Auto Scaling capabilities and is highly available. The database should be able to handle any changes in data attributes over time, in case the company updates the data feed from its IoT devices. The database must provide the capability to output a continuous stream with details of any changes to the underlying data.

As a Solutions Architect, which database will you recommend?

Amazon Relational Database Service (Amazon RDS)

Amazon Aurora

Amazon Redshift

Amazon DynamoDB

Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items.

---

The engineering team at a weather tracking company wants to enhance the performance of its relational database and is looking for a caching solution that supports geospatial data.

As a solutions architect, which of the following solutions will you suggest?

Use Amazon DynamoDB Accelerator (DAX)

Use Amazon ElastiCache for Redis

Use AWS Global Accelerator

Use Amazon ElastiCache for Memcached

Memcached para multithreaded, para geospatial redis.

---

While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the internet using the Internet Gateway.

Which conditions should be met for internet connectivity to be established? (Select two)

The route table in the instance’s subnet should have a route to an Internet Gateway

The instance's subnet is associated with multiple route tables with conflicting configurations

The network access control list (network ACL) associated with the subnet must have rules to allow inbound and outbound traffic

The subnet has been configured to be public and has no access to the internet

The instance's subnet is not associated with any route table

Lo único que diferencia una subnet pública d euna privada es si esta asociada a un IGW para salir a internet.

---

You are a cloud architect at an IT company. The company has multiple enterprise customers that manage their own mobile applications that capture and send data to Amazon Kinesis Data Streams. They have been getting a `ProvisionedThroughputExceededException` exception. You have been contacted to help and upon analysis, you notice that messages are being sent one by one at a high rate.

Which of the following options will help with the exception while keeping costs at a minimum?

Decrease the Stream retention duration

Use batch messages

Increase the number of shards

Use Exponential Backoff

When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.

---

A medium-sized business has a taxi dispatch application deployed on an Amazon EC2 instance. Because of an unknown bug, the application causes the instance to freeze regularly. Then, the instance has to be manually restarted via the AWS management console.

Which of the following is the MOST cost-optimal and resource-efficient way to implement an automated solution until a permanent fix is delivered by the development team?

Use Amazon EventBridge events to trigger an AWS Lambda function to reboot the instance status every 5 minutes

Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, Amazon CloudWatch Alarm can publish to an Amazon Simple Notification Service (Amazon SNS) event which can then trigger an AWS lambda function. The AWS lambda function can use Amazon EC2 API to reboot the instance

Setup an Amazon CloudWatch alarm to monitor the health status of the instance. In case of an Instance Health Check failure, an EC2 Reboot CloudWatch Alarm Action can be used to reboot the instance

Use Amazon EventBridge events to trigger an AWS Lambda function to check the instance status every 5 minutes. In the case of Instance Health Check failure, the AWS lambda function can use Amazon EC2 API to reboot the instance

You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures (as opposed to the recover alarm action, which is suited for System Health Check failures).

---

You have just terminated an instance in the `us-west-1a` Availability Zone (AZ). The attached Amazon EBS volume is now available for attachment to other instances. An intern launches a new Linux Amazon EC2 instance in the `us-west-1b` Availability Zone (AZ) and is attempting to attach the Amazon EBS volume. The intern informs you that it is not possible and needs your help.

Which of the following explanations would you provide to them?

The Amazon EBS volume is encrypted

Amazon EBS volumes are Availability Zone (AZ) locked

The required IAM permissions are missing

Amazon EBS volumes are region locked

---

A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 terabytes of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 terabytes of very durable storage for storing media content and almost double of it, i.e. 900 terabytes for archival of legacy data.

As a Solutions Architect, which set of services will you recommend to meet these requirements?

Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage

Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage

Instance store a encontrarse físicamente atada a la instancia siempre va a poder tener más performance en cuanto a I/O

---

A silicon valley based startup helps its users legally sign highly confidential contracts. To meet the compliance guidelines, the startup must ensure that the signed contracts are encrypted using the AES-256 algorithm via an encryption key that is generated as well as managed internally. The startup is now migrating to AWS Cloud and would like the data to be encrypted on AWS. The startup wants to continue using their existing encryption key generation as well as key management mechanism.

What do you recommend?

Client-Side Encryption

SSE-S3

SSE-KMS

SSE-C

Para el AES-256 el SSE-C utiliza este algoritmo en concreto, si fuera otro algoritmo personalizado se utilizaría Client-Side Encryption.

---

The engineering team at an e-commerce company uses an AWS Lambda function to write the order data into a single DB instance Amazon Aurora cluster. The team has noticed that many order- writes to its Aurora cluster are getting missed during peak load times. The diagnostics data has revealed that the database is experiencing high CPU and memory consumption during traffic spikes. The team also wants to enhance the availability of the Aurora DB.

Which of the following steps would you combine to address the given scenario? (Select two)

Increase the concurrency of the AWS Lambda function so that the order-writes do not get missed during traffic spikes

Handle all read operations for your application by connecting to the reader endpoint of the Amazon Aurora cluster so that Aurora can spread the load for read-only connections across the Aurora replica

Create a replica Aurora instance in another Availability Zone to improve the availability as the replica can serve as a failover target

Use Amazon EC2 instances behind an Application Load Balancer to write the order data into Amazon Aurora cluster

Create a standby Aurora instance in another Availability Zone to improve the availability as the standby can serve as a failover target

---

A streaming solutions company is building a video streaming product by using an Application Load Balancer (ALB) that routes the requests to the underlying Amazon EC2 instances. The engineering team has noticed a peculiar pattern. The Application Load Balancer removes an instance from its pool of healthy instances whenever it is detected as unhealthy but the Auto Scaling group fails to kick-in and provision the replacement instance.

What could explain this anomaly?

The Auto Scaling group is using Amazon EC2 based health check and the Application Load Balancer is using ALB based health check

Both the Auto Scaling group and Application Load Balancer are using Amazon EC2 based health check

The Auto Scaling group is using ALB based health check and the Application Load Balancer is using Amazon EC2 based health check

Both the Auto Scaling group and Application Load Balancer are using ALB based health check

---

A financial services firm uses a high-frequency trading system and wants to write the log files into Amazon S3. The system will also read these log files in parallel on a near real-time basis. The engineering team wants to address any data discrepancies that might arise when the trading system overwrites an existing log file and then tries to read that specific log file.

Which of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?

A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data

A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object

A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data

A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data

---

A big data analytics company is using Amazon Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.

As a solutions architect, which of the following would you recommend for improving the performance for the given use-case?

Use Enhanced Fanout feature of Amazon Kinesis Data Streams

Swap out Amazon Kinesis Data Streams with Amazon Kinesis Data Firehose

Swap out Amazon Kinesis Data Streams with Amazon SQS FIFO queues

Swap out Amazon Kinesis Data Streams with Amazon SQS Standard queues

Enhanced fanout digamos que hace que se dediquen varios canales a cada consumidor disminuyendo la latencia y la carga de un solo canal.

---

The content division at a digital media agency has an application that generates a large number of files on Amazon S3, each approximately 10 megabytes in size. The agency mandates that the files be stored for 5 years before they can be deleted. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. The files contain critical business data that is not easy to reproduce, therefore, immediate accessibility is always required.

Which solution is the MOST cost-effective for the given use case?

Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 One Zone-IA 30 days after object creation. Delete the files 5 years after object creation

Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Delete the files 5 years after object creation

Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Glacier Flexible Retrieval 30 days after object creation. Delete the files 5 years after object creation

Set up an Amazon S3 bucket lifecycle policy to move files from Amazon S3 Standard to Amazon S3 Standard-IA 30 days after object creation. Archive the files to Amazon S3 Glacier Deep Archive 5 years after object creation

![image.png](Exam%205%2028b5bd7c8610815fa01ed16198413445/image.png)

![image.png](Exam%205%2028b5bd7c8610815fa01ed16198413445/image%201.png)

---

The engineering team at a retail company manages 3 Amazon EC2 instances that make read-heavy database requests to the Amazon RDS for the PostgreSQL database instance. As an AWS Certified Solutions Architect - Associate, you have been tasked to make the database instance resilient from a disaster recovery perspective.

Which of the following features will help you in disaster recovery of the database? (Select two)

Use cross-Region Read Replicas

Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups across multiple Regions

Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region

Use Amazon RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage

Use the database cloning feature of the Amazon RDS Database cluster

---

An IT company has built a custom data warehousing solution for a retail organization by using Amazon Redshift. As part of the cost optimizations, the company wants to move any historical data (any data older than a year) into Amazon S3, as the daily analytical reports consume data for just the last one year. However the analysts want to retain the ability to cross-reference this historical data along with the daily reports.

The company wants to develop a solution with the LEAST amount of effort and MINIMUM cost. As a solutions architect, which option would you recommend to facilitate this use-case?

Use the Amazon Redshift COPY command to load the Amazon S3 based historical data into Amazon Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift

Setup access to the historical data via Amazon Athena. The analytics team can run historical data queries on Amazon Athena and continue the daily reporting on Amazon Redshift. In case the reports need to be cross-referenced, the analytics team need to export these in flat files and then do further analysis

Use Amazon Redshift Spectrum to create Amazon Redshift cluster tables pointing to the underlying historical data in Amazon S3. The analytics team can then query this historical data to cross-reference with the daily reports from Redshift

Use AWS Glue ETL job to load the Amazon S3 based historical data into Redshift. Once the ad-hoc queries are run for the historic data, it can be removed from Amazon Redshift

**Redshift Spectrum** utiliza **servidores dedicados** fuera de tu **clúster de Redshift** para gestionar las consultas sobre **datos en S3**, lo que **libera capacidad de procesamiento** en tu clúster de Redshift.

**Redshift Spectrum** está diseñado para **reducir costos de almacenamiento** y permitir **consultas eficientes** sobre datos que no necesariamente necesitan estar dentro del clúster de **Amazon Redshift**.

**Usar Amazon Redshift Spectrum para crear tablas de Amazon Redshift que apunten a los datos históricos en S3**: ✅ **Opción recomendada**

- Esta opción es la más **eficiente y de menor costo**. **Redshift Spectrum** te permite crear **tablas externas** en **Redshift** que apunten a los datos almacenados en **S3**. De esta manera, los analistas pueden realizar consultas **directas** tanto sobre los **datos recientes** en Redshift como sobre los **datos históricos** en S3, todo en una misma consulta SQL. No necesitas mover los datos a Redshift, lo que reduce costos de almacenamiento y aumenta la eficiencia operativa.