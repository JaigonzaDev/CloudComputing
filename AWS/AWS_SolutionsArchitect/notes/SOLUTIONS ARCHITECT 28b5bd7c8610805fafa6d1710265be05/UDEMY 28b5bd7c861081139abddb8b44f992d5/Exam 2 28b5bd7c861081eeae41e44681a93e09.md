# Exam 2

An IT company provides Amazon Simple Storage Service (Amazon S3) bucket access to specific users within the same account for completing project specific work. With changing business requirements, cross-account S3 access requests are also growing every month. The company is looking for a solution that can offer user level as well as account-level access permissions for the data stored in Amazon S3 buckets.

As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?

Use Access Control Lists (ACLs)

Use Amazon S3 Bucket Policies

Use Security Groups

Use Identity and Access Management (IAM) policies

**Use Amazon S3 Bucket Policies**

Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources.

You can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requester‚Äôs IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys.

---

You have a team of developers in your company, and you would like to ensure they can quickly experiment with AWS Managed Policies by attaching them to their accounts, but you would like to prevent them from doing an escalation of privileges, by granting themselves the¬†`AdministratorAccess`¬†managed policy. How should you proceed?

Attach an IAM policy to your developers, that prevents them from attaching the¬†`AdministratorAccess`¬†policy

Put the developers into an IAM group, and then define an IAM permission boundary on the group that will restrict the managed policies they can attach to themselves

Create a Service Control Policy (SCP) on your AWS account that restricts developers from attaching themselves the¬†`AdministratorAccess`¬†policy

For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves

**For each developer, define an IAM permission boundary that will restrict the managed policies they can attach to themselves**

AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. Here we have to use an IAM permission boundary. They can only be applied to roles or users, not IAM groups.

---

A company has historically operated only in the¬†`us-east-1`¬†region and stores encrypted data in Amazon S3 using SSE-KMS. As part of enhancing its security posture as well as improving the backup and recovery architecture, the company wants to store the encrypted data in Amazon S3 that is replicated into the¬†`us-west-1`¬†AWS region. The security policies mandate that the data must be encrypted and decrypted using the same key in both AWS regions.

Which of the following represents the best solution to address these requirements?

Create an Amazon CloudWatch scheduled rule to invoke an AWS Lambda function to copy the daily data from the source bucket in¬†`us-east-1`¬†region to the destination bucket in¬†`us-west-1`¬†region. Provide AWS KMS key access to the AWS Lambda function for encryption and decryption operations on the data in the source and destination Amazon S3 buckets

Create a new Amazon S3 bucket in the¬†`us-east-1`¬†region with replication enabled from this new bucket into another bucket in¬†`us-west-1`¬†region. Enable SSE-KMS encryption on the new bucket in¬†`us-east-1`¬†region by using an AWS KMS multi-region key. Copy the existing data from the current Amazon S3 bucket in¬†`us-east-1`¬†region into this new Amazon S3 bucket in¬†`us-east-1`¬†region

Enable replication for the current bucket in¬†`us-east-1`¬†region into another bucket in¬†`us-west-1`¬†region. Share the existing AWS KMS key from¬†`us-east-1`¬†region to¬†`us-west-1`¬†region

Change the AWS KMS single region key used for the current Amazon S3 bucket into an AWS KMS multi-region key. Enable Amazon S3 batch replication for the existing data in the current bucket in¬†`us-east-1`¬†region into another bucket in¬†`us-west-1`¬†region

AWS KMS supports multi-region keys, which are AWS KMS keys in different AWS regions that can be used interchangeably ‚Äì as though you had the same key in multiple regions. Each set of related multi-region keys has the same key material and key ID, so you can encrypt data in one AWS region and decrypt it in a different AWS region without re-encrypting or making a cross-region call to AWS KMS.

You can use multi-region AWS KMS keys in Amazon S3. However, Amazon S3 currently treats multi-region keys as though they were single-region keys, and does not use the multi-region features of the key.

---

A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.

Which of the following is the MOST cost-effective way of isolating their Amazon Elastic Compute Cloud (Amazon EC2)instances to a single tenant?

On-Demand Instances

Spot Instances

Dedicated Hosts

Dedicated Instances

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances

Dedicated Hosts te da servidor y control del hardware subyacente. Dedicates instance te dan hardware isolated pero tu no tienes control del hardware ni el hardware que te dan.

---

A weather forecast agency collects key weather metrics across multiple cities in the US and sends this data in the form of key-value pairs to AWS Cloud at a one-minute frequency.

As a solutions architect, which of the following AWS services would you use to build a solution for processing and then reliably storing this data with high availability? (Select two)

AWS Lambda

Amazon Redshift

Amazon RDS

Amazon DynamoDB

Amazon ElastiCache

Lambda para procesar, DynamoDb para guardar claves llave-valor.

Redshift no guarda claves llave-valor.

---

Amazon EC2 Auto Scaling needs to terminate an instance from Availability Zone (AZ)¬†`us-east-1a`¬†as it has the most number of instances amongst the Availability Zone (AZs) being used currently. There are 4 instances in the Availability Zone (AZ)¬†`us-east-1a`¬†like so: Instance A has the oldest launch template, Instance B has the oldest launch configuration, Instance C has the newest launch configuration and Instance D is closest to the next billing hour.

Which of the following instances would be terminated per the default termination policy?

Instance A

Instance C

Instance D

Instance B

1. Determina si es On-Demand o es SPot Instance  (elegira la prefered instance).
2. Determina si las instancias utiliza plantillas launch o configurate antiguas. (Primero configuration y luego launh).
3. Por √∫ltimo determina cual es son las √∫ltimas cerca del billing hour no las termina ya que puede ofrecer descuento por uso.

**Instance B**

Per the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A. Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination and Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority.

---

An HTTP application is deployed on an Auto Scaling Group, is accessible from an Application Load Balancer (ALB) that provides HTTPS termination, and accesses a PostgreSQL database managed by Amazon RDS.

How should you configure the security groups? (Select three)

The security group of the Application Load Balancer should have an inbound rule from anywhere on port 80

The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443

The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 80

The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432

The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80

The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Amazon RDS database on port 5432

El load balancer

**The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432**

**The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80**

**The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443**

A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, we use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group. When we decide whether to allow traffic to reach an instance, we evaluate all the rules from all the security groups that are associated with the instance.

The following are the characteristics of security group rules: 1. By default, security groups allow all outbound traffic. 2. Security group rules are always permissive; you can't create rules that deny access. 3. Security groups are stateful

PostgreSQL port = 5432 HTTP port = 80 HTTPS port = 443

The traffic goes like this : The client sends an HTTPS request to ALB on port 443. This is handled by the rule - "The security group of the Application Load Balancer should have an inbound rule from anywhere on port 443"

The Application Load Balancer then forwards the request to one of the Amazon EC2 instances. This is handled by the rule - "The security group of the Amazon EC2 instances should have an inbound rule from the security group of the Application Load Balancer on port 80"

The Amazon EC2 instance further accesses the PostgreSQL database managed by Amazon RDS on port 5432. This is handled by the rule - "The security group of Amazon RDS should have an inbound rule from the security group of the Amazon EC2 instances in the Auto Scaling group on port 5432"

---

A startup has just developed a video backup service hosted on a fleet of Amazon EC2 instances. The Amazon EC2 instances are behind an Application Load Balancer and the instances are using Amazon Elastic Block Store (Amazon EBS) Volumes for storage. The service provides authenticated users the ability to upload videos that are then saved on the EBS volume attached to a given instance. On the first day of the beta launch, users start complaining that they can see only some of the videos in their uploaded videos backup. Every time the users log into the website, they claim to see a different subset of their uploaded videos.

Which of the following is the MOST optimal solution to make sure that users can view all the uploaded videos? (Select two)

Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos

Write a one time job to copy the videos from all Amazon EBS volumes to Amazon RDS and then modify the application to use Amazon RDS for storing the videos

Write a one time job to copy the videos from all Amazon EBS volumes to Amazon DynamoDB and then modify the application to use Amazon DynamoDB for storing the videos

Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos

Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 Glacier Deep Archive and then modify the application to use Amazon S3 Glacier Deep Archive for storing the videos

el problema con **EBS** (Elastic Block Store) es que **no se puede compartir entre instancias EC2 directamente**. Cada volumen de EBS est√° asociado a una √∫nica instancia EC2 a la vez. Esto significa que si est√°s utilizando vol√∫menes EBS en las instancias dentro de tu grupo de Auto Scaling, cada instancia tendr√≠a su propio volumen de EBS adjunto, lo que genera los siguientes problemas:

1. **Acceso exclusivo a una sola instancia**: Un volumen EBS solo puede ser montado y accedido por una instancia EC2 en un momento dado. No puedes montar el mismo volumen EBS en varias instancias EC2 simult√°neamente. Esto crea un problema si deseas que todas las instancias EC2 puedan acceder a los mismos datos (como los videos subidos por los usuarios).
2. **Datos aislados**: Dado que cada instancia tiene su propio volumen EBS, los datos almacenados en EBS solo estar√°n disponibles para la instancia espec√≠fica a la que est√° adjunto el volumen. Esto significa que si un usuario carga un video en una instancia, solo esa instancia puede acceder a dicho video, lo que genera la inconsistencia que mencionas (al ver diferentes subconjuntos de videos seg√∫n la instancia).

**Write a one time job to copy the videos from all Amazon EBS volumes to Amazon S3 and then modify the application to use Amazon S3 standard for storing the videos**

**Mount Amazon Elastic File System (Amazon EFS) on all Amazon EC2 instances. Write a one time job to copy the videos from all Amazon EBS volumes to Amazon EFS. Modify the application to use Amazon EFS for storing the videos**

Amazon Elastic Block Store (EBS) is an easy to use, high-performance block storage service designed for use with Amazon Elastic Compute Cloud (EC2) for both throughput and transaction-intensive workloads at any scale.

Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.

Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.

As Amazon EBS volumes are attached locally to the Amazon EC2 instances, therefore the uploaded videos are tied to specific Amazon EC2 instances. Every time the user logs in, they are directed to a different instance and therefore their videos get dispersed across multiple EBS volumes. The correct solution is to use either Amazon S3 or Amazon EFS to store the user videos.

---

You would like to use AWS Snowball to move on-premises backups into a long term archival tier on AWS. Which solution provides the MOST cost savings?

Create a AWS Snowball job and target an Amazon S3 Glacier Deep Archive Vault

Create an AWS Snowball job and target a Amazon S3 Glacier Vault

Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier Deep Archive on the same day

Create an AWS Snowball job and target an Amazon S3 bucket. Create a lifecycle policy to transition this data to Amazon S3 Glacier on the same day

You can't move data directly from AWS Snowball into Amazon S3 Glacier, you need to go through Amazon S3 first, and then use a lifecycle policy. So this option is correct.

S3 Glacier Deep Archive provides more cost savings than Amazon S3 Glacier

---

A retail company wants to share sensitive accounting data that is stored in an Amazon RDS database instance with an external auditor. The auditor has its own AWS account and needs its own copy of the database.

Which of the following would you recommend to securely share the database with the auditor?

Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key

Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket

Set up a read replica of the database and configure IAM standard database authentication to grant the auditor access

Export the database contents to text files, store the files in Amazon S3, and create a new IAM user for the auditor with access to that bucket

**Create an encrypted snapshot of the database, share the snapshot, and allow access to the AWS Key Management Service (AWS KMS) encryption key**

You can share the AWS Key Management Service (AWS KMS) key that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS Key with another AWS account by adding the other account to the AWS KMS key policy.

Making an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case.

Incorrect options:

**Create a snapshot of the database in Amazon S3 and assign an IAM role to the auditor to grant access to the object in that bucket**¬†- Amazon RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the Amazon RDS instance is located. Amazon RDS stores these on your behalf and you do not have direct access to these snapshots in Amazon S3, so it's not possible to grant access to the snapshot objects in Amazon S3.

---

A company has many Amazon Virtual Private Cloud (Amazon VPC) in various accounts, that need to be connected in a star network with one another and connected with on-premises networks through AWS Direct Connect.

What do you recommend?

AWS PrivateLink

Virtual private gateway (VGW)

AWS Transit Gateway

VPC Peering Connection

Transit Gateway te deja conectar el entorno On-premises con tu VPC aparte de poder conectar varias VPCS entre s√≠.

**AWS Transit Gateway**

AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. So, this is a perfect use-case for the Transit Gateway.

---

A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of Amazon EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.

Which of the following options represents the best solution for the given requirements?

Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class

Amazon Elastic File System (EFS) Standard‚ÄìIA storage class

Amazon Elastic File System (EFS) Standard storage class

Amazon Elastic Block Store (EBS)

**Amazon Elastic File System (EFS) Standard‚ÄìIA storage class**¬†- Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances.

The Amazon S3 Standard‚ÄìIA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed.

---

An application is currently hosted on four Amazon EC2 instances (behind Application Load Balancer) deployed in a single Availability Zone (AZ). To maintain an acceptable level of end-user experience, the application needs at least 4 instances to be always available.

As a solutions architect, which of the following would you recommend so that the application achieves high availability with MINIMUM cost?

Deploy the instances in one Availability Zones. Launch two instances in the Availability Zone (AZ)

Deploy the instances in two Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)

Deploy the instances in three Availability Zones (AZs). Launch two instances in each Availability Zone (AZ)

Deploy the instances in two Availability Zones (AZs). Launch four instances in each Availability Zone (AZ)

---

A systems administrator has created a private hosted zone and associated it with a Virtual Private Cloud (VPC). However, the Domain Name System (DNS) queries for the private hosted zone remain unresolved.

As a Solutions Architect, can you identify the Amazon Virtual Private Cloud (Amazon VPC) options to be configured in order to get the private hosted zone to work?

Fix conflicts between your private hosted zone and any Resolver rule that routes traffic to your network for the same domain name, as it results in ambiguity over the route to be taken

Enable DNS hostnames and DNS resolution for private hosted zones

Remove any overlapping namespaces for the private and public hosted zones

Fix the Name server (NS) record and Start Of Authority (SOA) records that may have been created with wrong configurations

---

A financial services company has developed its flagship application on AWS Cloud with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key.

Which of the following Amazon S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?

Server-Side Encryption with Amazon S3 managed keys (SSE-S3)

Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3

Server-Side Encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)

Server-Side Encryption with Customer-Provided Keys (SSE-C)

‚ÄúThe company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption key.‚Äù

 Con Server-Side Encryption with Customer-Provided Keys (SSE-C) tu manejas las encriptions keys y s3 gestiona la encripci√≥n cuando escribes en disco y cuando haces la decription para ver los objetos.

---

A silicon valley based startup has a content management application with the web-tier running on Amazon EC2 instances and the database tier running on Amazon Aurora. Currently, the entire infrastructure is located in¬†`us-east-1`¬†region. The startup has 90% of its customers in the US and Europe. The engineering team is getting reports of deteriorated application performance from customers in Europe with high application load time.

As a solutions architect, which of the following would you recommend addressing these performance issues? (Select two)

**Create Amazon Aurora read replicas in the¬†`eu-west-1`¬†region**

**Setup another fleet of Amazon EC2 instances for the web tier in the¬†`eu-west-1`¬†region. Enable geolocation routing policy in Amazon Route 53**

**Create Amazon Aurora Multi-AZ standby instance in the¬†`eu-west-1`¬†region**

**Setup another fleet of Amazon EC2 instances for the web tier in the¬†`eu-west-1`¬†region. Enable failover routing policy in Amazon Route 53**

**Setup another fleet of Amazon EC2 instances for the web tier in the¬†`eu-west-1`¬†region. Enable latency routing policy in Amazon Route 53**

---

A big-data consulting firm is working on a client engagement where the extract, transform, and load (ETL) workloads are currently handled via a Hadoop cluster deployed in the on-premises data center. The client wants to migrate their ETL workloads to AWS Cloud. The AWS Cloud solution needs to be highly available with about 50 Amazon Elastic Compute Cloud (Amazon EC2) instances per Availability Zone (AZ).

As a solutions architect, which of the following Amazon EC2 placement groups would you recommend for handling the distributed ETL workload?

Partition placement group

Spread placement group

Both Spread placement group and Partition placement group

Cluster placement group

Depending on the type of workload, you can create a placement group using one of the following placement strategies:

- **Cluster**¬†‚Äì Packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of high-performance computing (HPC) applications.
- **Partition**¬†‚Äì Spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.
- **Spread**¬†‚Äì Strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.

---

Your company has a monthly big data workload, running for about 2 hours, which can be efficiently distributed across multiple servers of various sizes, with a variable number of CPUs. The solution for the workload should be able to withstand server failures.

Which is the MOST cost-optimal solution for this workload?

Run the workload on a Spot Fleet

Run the workload on Dedicated Hosts

Run the workload on Reserved Instances (RI)

Run the workload on Spot Instances

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html

**Run the workload on a Spot Fleet**

The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.

A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Spot Instances provide great cost efficiency, but we need to select an instance type in advance. In this case, we want to use the most cost-optimal option and leave the selection of the cheapest spot instance to a Spot Fleet request, which can be optimized with the¬†`lowestPrice`¬†strategy

**Run the workload on Spot Instances**¬†- A Spot Instance is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused Amazon EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. Only spot fleets can maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated, so spot instances, by themselves, are not the right fit for this use-case.

---

The engineering team at a logistics company has noticed that the Auto Scaling group (ASG) is not terminating an unhealthy Amazon EC2 instance.

As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)

The Amazon EC2 instance could be a spot instance type, which cannot be terminated by the Auto Scaling group (ASG)

**The instance has failed the Elastic Load Balancing (ELB) health check status**

**A user might have updated the configuration of the Auto Scaling group (ASG) and increased the minimum number of instances forcing ASG to keep all instances alive**

**The health check grace period for the instance has not expired**

**The instance maybe in Impaired status**

**A custom health check might have failed. The Auto Scaling group (ASG) does not terminate instances that are set unhealthy by custom checks**

**The health check grace period for the instance has not expired**

Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on Amazon EC2 status checks and Elastic Load Balancing (ELB) health checks until the health check grace period expires.

**The instance maybe in Impaired status**

Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch.

**The instance has failed the Elastic Load Balancing (ELB) health check status**

By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB.

---

A social media application is hosted on an Amazon EC2 fleet running behind an Application Load Balancer. The application traffic is fronted by an Amazon CloudFront distribution. The engineering team wants to decouple the user authentication process for the application, so that the application servers can just focus on the business logic.

As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?

Use Amazon Cognito Authentication via Cognito Identity Pools for your Amazon CloudFront distribution

Use Amazon Cognito Authentication via Cognito User Pools for your Amazon CloudFront distribution

Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer

Use Amazon Cognito Authentication via Cognito Identity Pools for your Application Load Balancer

**Use Amazon Cognito Authentication via Cognito User Pools for your Application Load Balancer**

Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules.

### **üìå M√©todos de autenticaci√≥n y seguridad para CloudFront**

‚úÖ **AWS Lambda@Edge o CloudFront Functions**

- Puedes usar **Lambda@Edge** para interceptar solicitudes y validar tokens de Cognito antes de servir contenido.
- Requiere desarrollo, pero es flexible para autenticaci√≥n y autorizaci√≥n personalizadas.

‚úÖ **Signed URLs y Signed Cookies**

- CloudFront permite controlar el acceso con **URLs firmadas** o **cookies firmadas**.
- Se pueden generar en un backend autenticado con Cognito y servirlas a los usuarios.
- Ideal para restringir acceso a ciertos archivos o videos.

‚úÖ **AWS WAF (Web Application Firewall)**

- No autentica usuarios directamente, pero **protege contra ataques** como SQL Injection, XSS, etc.
- Se usa para reforzar la seguridad de una app en CloudFront.

‚úÖ **OAC (Origin Access Control) o OAI (Origin Access Identity) para S3**

- Para restringir acceso directo a un bucket de S3, asegurando que solo CloudFront pueda servir los archivos

---

A media company is migrating its flagship application from its on-premises data center to AWS for improving the application's read-scaling capability as well as its availability. The existing architecture leverages a Microsoft SQL Server database that sees a heavy read load. The engineering team does a full copy of the production database at the start of the business day to populate a dev database. During this period, application users face high latency leading to a bad user experience.

The company is looking at alternate database options and migrating database engines if required. What would you suggest?

Leverage Amazon RDS for MySQL with a Multi-AZ deployment and use the standby instance as the dev database

Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and create the dev database by restoring from the automated backups of Amazon Aurora

Leverage Amazon RDS for SQL server with a Multi-AZ deployment and read replicas. Use the read replica as the dev database

Leverage Amazon Aurora MySQL with Multi-AZ Aurora Replicas and restore the dev database via mysqldump

Aurora backs up your cluster volume automatically and retains restore data for the length of the backup retention period. Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written.

A read replica cannot be used as a dev database because it does not allow any database write operations.

---

An e-commerce company operates multiple AWS accounts and has interconnected these accounts in a hub-and-spoke style using the AWS Transit Gateway. Amazon Virtual Private Cloud (Amazon VPCs) have been provisioned across these AWS accounts to facilitate network isolation.

Which of the following solutions would reduce both the administrative overhead and the costs while providing shared access to services required by workloads in each of the VPCs?

Build a shared services Amazon Virtual Private Cloud (Amazon VPC)

Use VPCs connected with AWS Direct Connect

Use Transit VPC to reduce cost and share the resources across Amazon Virtual Private Cloud (Amazon VPCs)

Use Fully meshed VPC Peering connection

Lo que el ejercicio plantea es c√≥mo **optimizar la conectividad** entre las m√∫ltiples VPCs que ya est√°n conectadas con **AWS Transit Gateway**.

En lugar de que cada VPC tenga que replicar servicios comunes (lo que aumenta **costos** y **complejidad administrativa**), lo que se hace es **crear una VPC centralizada** llamada **"Shared Services VPC"**.

---

A retail company wants to rollout and test a blue-green deployment for its global application in the next 48 hours. Most of the customers use mobile phones which are prone to Domain Name System (DNS) caching. The company has only two days left for the annual Thanksgiving sale to commence.

As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame?

Use Elastic Load Balancing (ELB) to distribute traffic across deployments

Use Amazon Route 53 weighted routing to spread traffic across different deployments

Use AWS CodeDeploy deployment options to choose the right deployment

Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment

un despligue blue/green es viable hacerlo con un load balancer cuando se hacer en una misma regi√≥n o multi-region, y un despligue de global acclerator cuando se hace globalmente

**Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment**

AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of your internet applications. It provides two static anycast IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions.

AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed).

While relying on the DNS service is a great option for blue/green deployments, it may not fit use-cases that require a fast and controlled transition of the traffic. Some client devices and internet resolvers cache DNS answers for long periods; this DNS feature improves the efficiency of the DNS service as it reduces the DNS traffic across the Internet, and serves as a resiliency technique by preventing authoritative name-server overloads. The downside of this in blue/green deployments is that you don‚Äôt know how long it will take before all of your users receive updated IP addresses when you update a record, change your routing preference or when there is an application failure.

With AWS Global Accelerator, you can shift traffic gradually or all at once between the blue and the green environment and vice-versa without being subject to DNS caching on client devices and internet resolvers, traffic dials and endpoint weights changes are effective within seconds.

**Use Elastic Load Balancing (ELB) to distribute traffic across deployments**¬†- Elastic Load Balancing (ELB) can distribute traffic across healthy instances. You can also use the Application Load Balancers weighted target groups feature for blue/green deployments as it does not rely on the DNS service. In addition you don‚Äôt need to create new ALBs for the green environment. As the use-case refers to a global application, so this option cannot be used for a multi-Region solution which is needed for the given requirement.

---

You have multiple AWS accounts within a single AWS Region managed by AWS Organizations and you would like to ensure all Amazon EC2 instances in all these accounts can communicate privately. Which of the following solutions provides the capability at the CHEAPEST cost?

Create a Private Link between all the Amazon EC2 instances

Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together

Create a VPC peering connection between all virtual private cloud (VPCs)

Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager

creas una VPC con recursos que necesitan acceder las distintas cuentas de las organizacioens y compartes las subnets apra dar acceso

**Create a virtual private cloud (VPC) in an account and share one or more of its subnets with the other accounts using Resource Access Manager**

AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge.

The correct solution is to share the subnet(s) within a VPC using RAM. This will allow all Amazon EC2 instances to be deployed in the same VPC (although from different accounts) and easily communicate with one another.

**Create an AWS Transit Gateway and link all the virtual private cloud (VPCs) in all the accounts together**¬†- AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. A Transit Gateway will work but will be an expensive solution. Here we want to minimize cost.

---

An engineering team wants to examine the feasibility of the¬†`user data`¬†feature of Amazon EC2 for an upcoming project.

Which of the following are true about the Amazon EC2 user data configuration? (Select two)

**When an instance is running, you can update user data by using root user credentials**

By default, user data runs only during the boot cycle when you first launch an instance

By default, scripts entered as user data are executed with root user privileges

By default, scripts entered as user data do not have root user privileges for executing

By default, user data is executed every time an Amazon EC2 instance is re-started

El User Data es el script que pones antes de empezar una instancia EC2 para que lo ejecute.

---

A media company has created an AWS Direct Connect connection for migrating its flagship application to the AWS Cloud. The on-premises application writes hundreds of video files into a mounted NFS file system daily. Post-migration, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. Before the migration cutover, the company must build a process that will replicate the newly created on-premises video files to the Amazon EFS file system.

Which of the following represents the MOST operationally efficient way to meet this requirement?

Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using a VPC gateway endpoint for Amazon S3. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system

Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an Amazon S3 bucket by using public VIF. Set up an AWS Lambda function to process event notifications from Amazon S3 and copy the video files from Amazon S3 to the Amazon EFS file system

Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours

Configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the AWS Direct Connect connection to an AWS VPC peering endpoint for Amazon EFS by using a private VIF. Set up an AWS DataSync scheduled task to send the video files to the Amazon EFS file system every 24 hours

You cannot use the Amazon S3 gateway endpoint to transfer data over the AWS Direct Connect connection from the on-premises systems to Amazon S3. So this option is incorrect.

To establish a private connection between your virtual private cloud (VPC) and the Amazon EFS API, you can create an interface VPC endpoint

---

An analytics company wants to improve the performance of its big data processing workflows running on Amazon Elastic File System (Amazon EFS). Which of the following performance modes should be used for Amazon EFS to address this requirement?

Bursting Throughput

Provisioned Throughput

General Purpose

Max I/O

https://docs.aws.amazon.com/efs/latest/ug/performance.html

Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.

Amazon EFS offers two performance modes, General Purpose and Max I/O.

- **General Purpose mode**¬†has the lowest per-operation latency and is the default performance mode for file systems. One Zone file systems always use the General Purpose performance mode. For faster performance, we recommend always using General Purpose performance mode.
- **Max I/O mode**¬†is a previous generation performance type that is designed for highly parallelized workloads that can tolerate higher latencies than the General Purpose mode. Max I/O mode is not supported for One Zone file systems or file systems that use Elastic throughput.

---

A big data consulting firm needs to set up a data lake on Amazon S3 for a Health-Care client. The data lake is split in raw and refined zones. For compliance reasons, the source data needs to be kept for a minimum of 5 years. The source data arrives in the raw zone and is then processed via an AWS Glue based extract, transform, and load (ETL) job into the refined zone. The business analysts run ad-hoc queries only on the data in the refined zone using Amazon Athena. The team is concerned about the cost of data storage in both the raw and refined zones as the data is increasing at a rate of 1 terabyte daily in each zone.

As a solutions architect, which of the following would you recommend as the MOST cost-optimal solution? (Select two)

Setup a lifecycle policy to transition the raw zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation

Create an AWS Lambda function based job to delete the raw zone data after 1 day

Setup a lifecycle policy to transition the refined zone data into Amazon S3 Glacier Deep Archive after 1 day of object creation

Use AWS Glue ETL job to write the transformed data in the refined zone using a compressed file format

Use AWS Glue ETL job to write the transformed data in the refined zone using CSV format

Es decir primero coges y guardas los datos raw y cuando se vaya a procesar ya usas el glue y lo guardas normal ya que se va a estar usando esa informaci√≥n.

AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You cannot transition the refined zone data into Amazon S3 Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Therefore, the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone.

---

An IT company has built a solution wherein an Amazon Redshift cluster writes data to an Amazon S3 bucket belonging to a different AWS account. However, it is found that the files created in the Amazon S3 bucket using the UNLOAD command from the Amazon Redshift cluster are not even accessible to the Amazon S3 bucket owner.

What could be the reason for this denial of permission for the bucket owner?

The owner of an Amazon S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress

When two different AWS accounts are accessing an Amazon S3 bucket, both the accounts must share the bucket policies. An erroneous policy can lead to such permission failures

By default, an Amazon S3 object is owned by the AWS account that uploaded it. So the Amazon S3 bucket owner will not implicitly have access to the objects written by the Amazon Redshift cluster

When objects are uploaded to Amazon S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. This issue seems to be due to an upload error that can be fixed by providing manual access from AWS console

---

The engineering team at an e-commerce company is working on cost optimizations for Amazon Elastic Compute Cloud (Amazon EC2) instances. The team wants to manage the workload using a mix of on-demand and spot instances across multiple instance types. They would like to create an Auto Scaling group with a mix of these instances.

Which of the following options would allow the engineering team to provision the instances for this use-case?

You can use a launch configuration or a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost

You can only use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost

You can neither use a launch configuration nor a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost

You can only use a launch template to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost

A launch template is similar to a launch configuration, in that it specifies instance configuration information such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances. Also, defining a launch template instead of a launch configuration allows you to have multiple versions of a template.

With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost. Hence this is the correct option.

A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.

You cannot use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. Therefore both these options are incorrect.

---

A developer needs to implement an AWS Lambda function in AWS account A that accesses an Amazon Simple Storage Service (Amazon S3) bucket in AWS account B.

As a Solutions Architect, which of the following will you recommend to meet this requirement?

Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the AWS Lambda function's execution role. Make sure that the bucket policy also grants access to the AWS Lambda function's execution role

The Amazon S3 bucket owner should make the bucket public so that it can be accessed by the AWS Lambda function in the other AWS account

Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket

AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda

If the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Therefore, this is the right way of giving access to AWS Lambda for the given use-case.

**Create an IAM role for the AWS Lambda function that grants access to the Amazon S3 bucket. Set the IAM role as the Lambda function's execution role and that would give the AWS Lambda function cross-account access to the Amazon S3 bucket**¬†- When the execution role of AWS Lambda and Amazon S3 bucket to be accessed are from different accounts, then you need to grant Amazon S3 bucket access permissions to the IAM role and also ensure that the bucket policy grants access to the AWS Lambda function's execution role.